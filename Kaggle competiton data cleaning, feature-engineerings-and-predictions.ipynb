{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOUSE SALE PRICING PREDICTION\n",
    "## Author: Yujie Fu, Zhijie Chen, Jie Yang\n",
    "\n",
    "\n",
    "Hi folks! This is a beginners notebook that covers all the main steps necessary to complete a beginning Machine Learning project.\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:06.888548Z",
     "iopub.status.busy": "2021-10-12T02:45:06.888177Z",
     "iopub.status.idle": "2021-10-12T02:45:06.895612Z",
     "shell.execute_reply": "2021-10-12T02:45:06.894665Z",
     "shell.execute_reply.started": "2021-10-12T02:45:06.888514Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.linear_model import LinearRegression # Linear regression\n",
    "from sklearn.impute import KNNImputer # KnnImputer for missing value\n",
    "from sklearn.model_selection import train_test_split # for splitting dataset into training sub dataset and validation subdataset\n",
    "from sklearn.model_selection import GridSearchCV # Tune hyper-parameters\n",
    "from sklearn.pipeline import Pipeline # No need to scale data manually\n",
    "from scipy.stats import skew # For skewness\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures # Min-max scaling, standardized scaling\n",
    "from sklearn.neural_network import MLPRegressor # Neural network\n",
    "from sklearn.linear_model import Ridge # Ridge regression\n",
    "from sklearn.ensemble import RandomForestRegressor # Random Forest\n",
    "from xgboost import XGBRegressor # Boosting tree >> XGBRegressor\n",
    "from catboost import CatBoostRegressor # Boosting tree >> Cat Boosting regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor # Knn\n",
    "from sklearn.linear_model import Ridge #Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DATA PREPROCESSING\n",
    "\n",
    "Now, we need to preprocess data for the modeling part. The main steps are:\n",
    "\n",
    "- **Looking at potential NaN**\n",
    "    - numerical features : knn imputer using whole dataset (train+test), k=5\n",
    "    - categorical features: treat null values as new category\n",
    "- **Dealing with categorical features (e.g. Dummy coding)**\n",
    "- **Handle skewness**\n",
    "    - log transform on skewed values with skewness > 0.75 as well as targeted variable SalePrice\n",
    "- **Normalization (combined in data modeling)**\n",
    "    - standardized scaling\n",
    "    - min-max scaling\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:06.898225Z",
     "iopub.status.busy": "2021-10-12T02:45:06.897728Z",
     "iopub.status.idle": "2021-10-12T02:45:06.958016Z",
     "shell.execute_reply": "2021-10-12T02:45:06.95704Z",
     "shell.execute_reply.started": "2021-10-12T02:45:06.898183Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "data_train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n",
    "data_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
    "\n",
    "# Record targeted variable before dropping\n",
    "yTr = data_train[\"SalePrice\"]\n",
    "\n",
    "# SalePrice is highly skewed, log transform it\n",
    "yTr = np.log1p(yTr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:06.959662Z",
     "iopub.status.busy": "2021-10-12T02:45:06.959408Z",
     "iopub.status.idle": "2021-10-12T02:45:06.976557Z",
     "shell.execute_reply": "2021-10-12T02:45:06.97567Z",
     "shell.execute_reply.started": "2021-10-12T02:45:06.959632Z"
    }
   },
   "outputs": [],
   "source": [
    "#kick out null_values>threshold   FireplaceQu,Alley,MiscFeature,Fence,PoolQC   all has already been moved in the drop features\n",
    "drop_null_threshold=0.3\n",
    "null_percentage=pd.DataFrame(data_train.isnull().sum()/data_train.shape[0],columns=['null_percentage'])\n",
    "drop_null_list=(null_percentage.loc[null_percentage['null_percentage']>drop_null_threshold,].index)  #this variable is the columns of the names for any column exceed the threshold\n",
    "\n",
    "#next time may be we can use the drop_null_list+['id']+['SalePrice'] to drop columns\n",
    "\n",
    "# Drop features\n",
    "data_train = data_train.drop(['Id','PoolQC','MiscFeature','Alley','Fence','FireplaceQu','SalePrice'],axis = 1)\n",
    "data_test = data_test.drop(['Id','PoolQC','MiscFeature','Alley','Fence','FireplaceQu'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:06.978497Z",
     "iopub.status.busy": "2021-10-12T02:45:06.978203Z",
     "iopub.status.idle": "2021-10-12T02:45:07.340311Z",
     "shell.execute_reply": "2021-10-12T02:45:07.339439Z",
     "shell.execute_reply.started": "2021-10-12T02:45:06.97847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare to build a whole dataset,null values will be filled based on whole data knn imputer\n",
    "data_whole = pd.concat([data_train, data_test])\n",
    "\n",
    "# Get categotical features names and numeric features name\n",
    "cat_feats = data_whole.dtypes[data_train.dtypes == \"object\"].index\n",
    "num_feats = data_whole.dtypes[data_train.dtypes != \"object\"].index\n",
    "\n",
    "data_whole_num = data_whole.loc[:,num_feats]\n",
    "\n",
    "# Fill out the null values using knn impute\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "data_whole_num = pd.DataFrame(imputer.fit_transform(data_whole_num), columns = data_whole_num.columns)\n",
    "data_whole.index = data_whole_num.index\n",
    "\n",
    "data_whole = pd.concat([data_whole_num, data_whole.loc[:,cat_feats]], axis = 1)\n",
    "\n",
    "# Categorical variables will be treated as new category\n",
    "data_whole.fillna(\"NONE\")\n",
    "data_whole = pd.get_dummies(data_whole, dtype = \"Float64\")\n",
    "\n",
    "# Handle skewness\n",
    "skew_threshold = 0.75\n",
    "\n",
    "skewed_feats = data_whole.loc[:, num_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "skewed_feats = skewed_feats[skewed_feats > skew_threshold]\n",
    "skewed_feats = skewed_feats.index.tolist()\n",
    "data_whole[skewed_feats] = np.log1p(data_whole[skewed_feats]) # log1p: log(1+x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Outlier to Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.342574Z",
     "iopub.status.busy": "2021-10-12T02:45:07.342323Z",
     "iopub.status.idle": "2021-10-12T02:45:07.347354Z",
     "shell.execute_reply": "2021-10-12T02:45:07.346438Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.342545Z"
    }
   },
   "outputs": [],
   "source": [
    "check outlier and change into Nan\n",
    "outlier_threshold=1.7\n",
    "quan_columns=[]\n",
    "for col in quan_columns:\n",
    "    Q1=np.percentile(data_train[col],25)\n",
    "    Q3=np.percentile(data_train[col],75)\n",
    "    lower_outlier_bound=Q1-outlier_threshold*(Q3-Q1)\n",
    "    upper_outlier_bound=Q3+outlier_threshold*(Q3-Q1)\n",
    "    outlier_list_col = data_train[(data_train[col] <  lower_outlier_bound) | (data_train[col] > upper_outlier_bound)].index\n",
    "    data_train[col][outlier_list_col]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.348558Z",
     "iopub.status.busy": "2021-10-12T02:45:07.348355Z",
     "iopub.status.idle": "2021-10-12T02:45:07.363477Z",
     "shell.execute_reply": "2021-10-12T02:45:07.362366Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.348532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare xTr, xTe for model building\n",
    "xTr = data_whole.loc[0:len(data_train)-1,:]\n",
    "xTe = data_whole.loc[len(data_train):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DATA MODELING\n",
    "- **Knn regression**\n",
    "- **Random forest**\n",
    "- **Boosting trees**\n",
    "    - xgb boosting\n",
    "    - cat boosting\n",
    "- **Neural network**\n",
    "\n",
    "You can uncomment block of codes to run the model. Personally, I highly recommend run one model at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knn regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.365726Z",
     "iopub.status.busy": "2021-10-12T02:45:07.365404Z",
     "iopub.status.idle": "2021-10-12T02:45:07.376282Z",
     "shell.execute_reply": "2021-10-12T02:45:07.375421Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.365684Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Features are standardized, tries k = 1...6.\n",
    "# estimator_knn = Pipeline(steps=[('scalar',StandardScaler()),('regressor', KNeighborsRegressor())])\n",
    "# parameters_knn = {'regressor__n_neighbors':[1,2,3,4,5,6]}\n",
    "# model_knn = GridSearchCV(estimator_knn, param_grid=parameters_knn, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# # train the model and get best parameters and scores\n",
    "# model_knn.fit(xTr,yTr)\n",
    "# print(\"Best estimator:\")\n",
    "# print(model_knn.best_estimator_)\n",
    "# print(\"Best validation RMSE:\")\n",
    "# print(np.sqrt(-model_knn.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat boosting regressor(Best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.378895Z",
     "iopub.status.busy": "2021-10-12T02:45:07.378629Z",
     "iopub.status.idle": "2021-10-12T02:45:07.390336Z",
     "shell.execute_reply": "2021-10-12T02:45:07.389355Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.378866Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator_cat_b = Pipeline(steps=[('scaler',StandardScaler()), ('cat_b',CatBoostRegressor(verbose=False))])\n",
    "# parameters_cat_b = {'cat_b__iterations': [6000],\n",
    "#                     'cat_b__learning_rate': [0.005],\n",
    "#                     'cat_b__depth': [4, 6, 10],\n",
    "#                     'cat_b__l2_leaf_reg': [1]}\n",
    "# model_cat_b = GridSearchCV(estimator_cat_b, param_grid=parameters_cat_b, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# # train the model and get best parameters and scores\n",
    "# model_cat_b.fit(xTr,yTr)\n",
    "# print(\"Best estimator:\")\n",
    "# print(model_cat_b.best_estimator_)\n",
    "# print(\"Best validation RMSE:\")\n",
    "# print(np.sqrt(-model_cat_b.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB boosting tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.392296Z",
     "iopub.status.busy": "2021-10-12T02:45:07.391974Z",
     "iopub.status.idle": "2021-10-12T02:45:07.405417Z",
     "shell.execute_reply": "2021-10-12T02:45:07.404223Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.392256Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator_xgb = Pipeline(steps=[('scaler',MinMaxScaler()), ('xgb', XGBRegressor())])\n",
    "# parameters_xgb = {'xgb__reg_alpha': [0.1,0.5],'xgb__learning_rate': [0.02,0.04], 'xgb__n_estimators' : [200,100,150], 'xgb__max_depth':[3,4,5]}\n",
    "# model_xgb = GridSearchCV(estimator_xgb, param_grid=parameters_xgb, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# # train the model and get best parameters and scores\n",
    "# model_xgb.fit(xTr,yTr)\n",
    "# print(\"Best estimator:\")\n",
    "# print(model_xgb.best_estimator_)\n",
    "# print(\"Best validation RMSE:\")\n",
    "# print(np.sqrt(-model_xgb.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.407471Z",
     "iopub.status.busy": "2021-10-12T02:45:07.407037Z",
     "iopub.status.idle": "2021-10-12T02:45:07.422615Z",
     "shell.execute_reply": "2021-10-12T02:45:07.42171Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.407425Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator_rf = Pipeline(steps=[('scaler',MinMaxScaler()), ('RF', RandomForestRegressor(criterion='mse', random_state=10, bootstrap=False))])\n",
    "# parameters_rf = {'RF__n_estimators': [200,250], 'RF__max_features': ['auto','sqrt']}\n",
    "# model_rf = GridSearchCV(estimator_rf, param_grid=parameters_rf, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# # train the model and get best parameters and scores\n",
    "# model_rf.fit(xTr,yTr)\n",
    "# print(\"Best estimator:\")\n",
    "# print(model_rf.best_estimator_)\n",
    "# print(\"Best validation RMSE:\")\n",
    "# print(np.sqrt(-model_rf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.425809Z",
     "iopub.status.busy": "2021-10-12T02:45:07.425498Z",
     "iopub.status.idle": "2021-10-12T02:45:07.435739Z",
     "shell.execute_reply": "2021-10-12T02:45:07.434687Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.425771Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator_nn = Pipeline(steps=[('scaler',StandardScaler()), ('mlp',MLPRegressor(max_iter = 3000))])\n",
    "# parameters_nn = {'mlp__hidden_layer_sizes': [(300,200,300),(300,200,50),(100,100,100)], 'mlp__alpha':[1,0.1,1e-2]}\n",
    "# model_nn = GridSearchCV(estimator_nn, param_grid=parameters_nn, cv=5, scoring='neg_mean_squared_error',verbose=0, n_jobs=-1)\n",
    "\n",
    "# # train the model and get best parameters and scores\n",
    "# model_nn.fit(xTr,yTr)\n",
    "# print(\"Best estimator:\")\n",
    "# print(model_nn.best_estimator_)\n",
    "# print(\"Best validation RMSE:\")\n",
    "# print(np.sqrt(-model_nn.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:07.438077Z",
     "iopub.status.busy": "2021-10-12T02:45:07.437204Z",
     "iopub.status.idle": "2021-10-12T02:45:08.509489Z",
     "shell.execute_reply": "2021-10-12T02:45:08.508477Z",
     "shell.execute_reply.started": "2021-10-12T02:45:07.43803Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator_ridge = Pipeline(steps=[('scaler',StandardScaler()), ('Ridge',Ridge())])\n",
    "parameters_ridge = {'Ridge__alpha': [0.2,0.4,0.6,0.8,1.0]}\n",
    "model_ridge = GridSearchCV(estimator_ridge, param_grid=parameters_ridge, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# train the model and get best parameters and scores\n",
    "model_ridge.fit(xTr,yTr)\n",
    "print(\"Best estimator:\")\n",
    "print(model_ridge.best_estimator_)\n",
    "print(\"Best validation RMSE:\")\n",
    "print(np.sqrt(-model_ridge.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:08.517359Z",
     "iopub.status.busy": "2021-10-12T02:45:08.5161Z",
     "iopub.status.idle": "2021-10-12T02:45:08.530735Z",
     "shell.execute_reply": "2021-10-12T02:45:08.529265Z",
     "shell.execute_reply.started": "2021-10-12T02:45:08.517299Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Get submission.csv, be ready to submit\n",
    "\n",
    "# # Convert SalePrice back\n",
    "# final = np.expm1(model_cat_b.predict(xTe)) # np.expm1: inverse of np.log1p\n",
    "\n",
    "# data_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
    "# out_df = pd.DataFrame()\n",
    "# out_df[\"Id\"] = data_test[\"Id\"]\n",
    "# out_df[\"SalePrice\"] = final\n",
    "# out_df\n",
    "# out_df.to_csv(\"./submission.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explotary procedure\n",
    "\n",
    "Uncomment to run codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:08.534142Z",
     "iopub.status.busy": "2021-10-12T02:45:08.533406Z",
     "iopub.status.idle": "2021-10-12T02:45:08.543609Z",
     "shell.execute_reply": "2021-10-12T02:45:08.542473Z",
     "shell.execute_reply.started": "2021-10-12T02:45:08.534092Z"
    }
   },
   "outputs": [],
   "source": [
    "# decision tree regressor_by Zhijie\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "estimator=DecisionTreeRegressor(random_state=22)\n",
    "\n",
    "#set parameter, here I only use max_depth as reference\n",
    "param={'max_depth':[3,4,5,6,7]}\n",
    "\n",
    "#use grid_search corssvalidation \n",
    "gc=GridSearchCV(estimator,param_grid=param,cv=5,scoring='neg_mean_squared_error')\n",
    "\n",
    "gc.fit(xTr,yTr)\n",
    "\n",
    "# clf=gc.best_estimator_.predict(xVal)\n",
    "# MSE=-gc.score(xVal,yVal)\n",
    "# RMSE=MSE**0.5\n",
    "# RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:08.548036Z",
     "iopub.status.busy": "2021-10-12T02:45:08.545504Z",
     "iopub.status.idle": "2021-10-12T02:45:08.557421Z",
     "shell.execute_reply": "2021-10-12T02:45:08.555542Z",
     "shell.execute_reply.started": "2021-10-12T02:45:08.547977Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "estimator=RandomForestRegressor(random_state=22)\n",
    "param={'n_estimators':[100,200,500,1000]}\n",
    "gc=GridSearchCV(estimator,param_grid=param,cv=5,scoring='neg_mean_squared_error')\n",
    "gc.fit(xTr,yTr)\n",
    "clf=gc.best_estimator_.predict(xVal)\n",
    "MSE=-gc.score(xVal,yVal)\n",
    "RMSE=MSE**0.5\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:45:08.562166Z",
     "iopub.status.busy": "2021-10-12T02:45:08.559522Z",
     "iopub.status.idle": "2021-10-12T02:45:08.580182Z",
     "shell.execute_reply": "2021-10-12T02:45:08.578723Z",
     "shell.execute_reply.started": "2021-10-12T02:45:08.562102Z"
    }
   },
   "outputs": [],
   "source": [
    "# KNN regressor by YJ\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV  #gridsearch for securing the best parameter\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "parameters={'n_neighbors':[1,3,5,7,9,11,13,15]}\n",
    "kng=KNeighborsRegressor()  #Note: here assigned parameter is not the best\n",
    "\n",
    "# Find best K with GridSearchCV\n",
    "clf=GridSearchCV(kng,parameters,cv=5)  #5折\n",
    "clf.fit(xTr,yTr)\n",
    "\n",
    "# Best parameter K is 7 here\n",
    "print(\"Best score/accuracy：%.2f\"%clf.best_score_,\"Best K:\",clf.best_params_)\n",
    "\n",
    "kng=KNeighborsRegressor(n_neighbors=7) #Using 7 as the K\n",
    "\n",
    "kng.fit(xVal,yVal)\n",
    "kng_prediction=kng.predict(xVal)\n",
    "np.mean((kng_prediction - yVal)**2)**0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
